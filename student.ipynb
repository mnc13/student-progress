{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13285974,"sourceType":"datasetVersion","datasetId":8420086}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==== Minimal, conflict-safe setup (Colab/Kaggle) ====\n# Installs only what we need, without touching core environment packages.\n\n%pip -q install --no-deps pyBKT==1.4.1\n%pip -q install --no-deps lightgbm==4.3.0\n%pip -q install --no-deps sentence-transformers==3.0.1\n%pip -q install --no-deps faiss-cpu==1.8.0.post1\n%pip -q install --no-deps jinja2==3.1.4\n\n# Optional: install transformers/torch ONLY if they’re missing (keeps environment stable)\ntry:\n    import torch, transformers  # noqa: F401\nexcept Exception:\n    %pip -q install --upgrade-strategy only-if-needed torch transformers\n\n# ---- Sanity checks & device setup ----\nimport warnings, os\nwarnings.filterwarnings(\"ignore\")\n\n# Core imports\nimport pandas as pd, numpy as np\nimport lightgbm as lgb\nfrom pyBKT.models import Model\nfrom sentence_transformers import SentenceTransformer\n\n# Device (GPU used by Sentence-Transformers if available)\ntry:\n    import torch\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nexcept Exception:\n    DEVICE = \"cpu\"\n\nprint(\"✅ Environment OK\")\nprint(\"  - PyBKT:\", Model.__module__)\nprint(\"  - LightGBM:\", lgb.__version__)\nprint(\"  - Torch device:\", DEVICE)\n\n# Quick embedding model warm-up (downloads to cache)\n_ = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\nprint(\"  - Sentence-Transformers ready\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-07T04:59:31.510593Z","iopub.execute_input":"2025-10-07T04:59:31.511180Z","iopub.status.idle":"2025-10-07T05:00:16.932700Z","shell.execute_reply.started":"2025-10-07T04:59:31.511151Z","shell.execute_reply":"2025-10-07T05:00:16.931978Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"name":"stderr","text":"2025-10-07 04:59:53.459650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759813193.690432      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759813193.762044      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✅ Environment OK\n  - PyBKT: pyBKT.models.Model\n  - LightGBM: 4.3.0\n  - Torch device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266f2c0fdd4642b4bf0f5fd80ff7ed10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e480e4191f32448db8762942aca38ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f882ef909ba423d85a4cd14c345d23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939669ca73404a68983ea8ac9f976f94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e4b13ef2e240d7a612d842112d12d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"200f93046dc24401a94050d0f9e1dc89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63065b0caee649cdb3fa71a29d10e3af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687c47f4b2e64e06b6310f3e1719ce02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee07f3e00894c199703c05609ba411a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b45460e8f5a48c3a1b779f097f24552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61ddf3d69488443cb7866e0f17a54563"}},"metadata":{}},{"name":"stdout","text":"  - Sentence-Transformers ready\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==== Locate dataset, (optional) copy to /kaggle/working, and load ALL CSVs ====\nfrom pathlib import Path\nimport shutil\nimport pandas as pd\n\n# Files we expect\nREQUIRED = {\n    \"students.csv\",\n    \"courses.csv\",\n    \"topics.csv\",\n    \"assessments.csv\",\n    \"engagement.csv\",\n    \"feedback.csv\",\n    \"course_summary_grades.csv\",\n}\n\n# 1) Find DATA folder\nDATA = Path(\".\")  # default: current dir (in case you've already copied files here)\ndef contains_required(dirpath: Path) -> bool:\n    try:\n        names = {p.name for p in dirpath.glob(\"*.csv\")}\n        return REQUIRED.issubset(names)\n    except Exception:\n        return False\n\nif not contains_required(DATA):\n    # Try Kaggle input mounts\n    for d in Path(\"/kaggle/input\").glob(\"*\"):\n        if contains_required(d):\n            DATA = d\n            break\n\nprint(\"Using DATA folder:\", DATA)\nprint(\"Files in DATA:\", sorted([p.name for p in DATA.glob('*.csv')]))\nassert contains_required(DATA), \"Could not find all required CSVs in DATA.\"\n\n# 2) (Optional) Copy inputs to /kaggle/working so artifacts land together\nCOPY_TO_WORKING = False  # set True if you want local copies\nif COPY_TO_WORKING:\n    DST = Path(\"/kaggle/working\")\n    DST.mkdir(parents=True, exist_ok=True)\n    for fname in REQUIRED:\n        shutil.copy(DATA/fname, DST/fname)\n    print(\"Copied to /kaggle/working\")\n\n# 3) Load ALL CSVs into dataframes\nstudents  = pd.read_csv(DATA/\"students.csv\")\ncourses   = pd.read_csv(DATA/\"courses.csv\")\ntopics    = pd.read_csv(DATA/\"topics.csv\")\nassess    = pd.read_csv(DATA/\"assessments.csv\", parse_dates=[\"timestamp\"])\nengage    = pd.read_csv(DATA/\"engagement.csv\")\nfeedback  = pd.read_csv(DATA/\"feedback.csv\")\ngrades    = pd.read_csv(DATA/\"course_summary_grades.csv\")\n\n# 4) Quick sanity print\nfor name, df in [\n    (\"students\", students),\n    (\"courses\", courses),\n    (\"topics\", topics),\n    (\"assessments\", assess),\n    (\"engagement\", engage),\n    (\"feedback\", feedback),\n    (\"grades\", grades),\n]:\n    print(f\"{name:12s} -> shape={df.shape}, cols={list(df.columns)[:6]}{'...' if df.shape[1]>6 else ''}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:03:59.154916Z","iopub.execute_input":"2025-10-07T05:03:59.155569Z","iopub.status.idle":"2025-10-07T05:03:59.263825Z","shell.execute_reply.started":"2025-10-07T05:03:59.155541Z","shell.execute_reply":"2025-10-07T05:03:59.263150Z"}},"outputs":[{"name":"stdout","text":"Using DATA folder: /kaggle/input/student-progress\nFiles in DATA: ['assessments.csv', 'course_summary_grades.csv', 'courses.csv', 'engagement.csv', 'feedback.csv', 'students.csv', 'topics.csv']\nstudents     -> shape=(200, 4), cols=['student_id', 'name', 'batch_year', 'department']\ncourses      -> shape=(20, 5), cols=['course_code', 'course_title', 'semester_no', 'course_type', 'credit_hours']\ntopics       -> shape=(119, 3), cols=['course_code', 'topic_id', 'topic_name']\nassessments  -> shape=(24000, 8), cols=['student_id', 'course_code', 'semester_no', 'assessment_type', 'topic_id', 'raw_score']...\nengagement   -> shape=(4000, 8), cols=['student_id', 'course_code', 'semester_no', 'attendance_percentage', 'lms_logins', 'forum_posts']...\nfeedback     -> shape=(4000, 6), cols=['student_id', 'course_code', 'semester_no', 'interest_rating', 'perceived_difficulty', 'comments']\ngrades       -> shape=(4000, 7), cols=['student_id', 'course_code', 'semester_no', 'course_type', 'credit_hours', 'final_percentage']...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================================\n# 2) Feature Engineering + New Policy Labels\n# Final% = 10% Att + 15% (quizzes+assign) + 25% Mid + 50% Final\n# =========================================================\nLETTER_BANDS = [(\"A+\",80,101),(\"A\",75,80),(\"A-\",70,75),(\"B+\",65,70),(\"B\",60,65),(\"B-\",55,60),\n                (\"C+\",50,55),(\"C\",45,50),(\"D\",40,45),(\"F\",0,40)]\ndef letter_from_pct(p):\n    for band, lo, hi in LETTER_BANDS:\n        if p>=lo and p<hi: return band\n    return \"F\"\n\ndef add_course_meta(df):\n    return df.merge(courses[[\"course_code\",\"semester_no\",\"course_type\",\"credit_hours\"]]\n                    .drop_duplicates(), on=[\"course_code\",\"semester_no\"], how=\"left\")\n\nassess[\"norm\"] = (assess[\"raw_score\"]/assess[\"max_score\"]).clip(0,1)\n\ndef build_assessment_wide(df):\n    piv = df.pivot_table(index=[\"student_id\",\"course_code\",\"semester_no\"],\n                         columns=\"assessment_type\", values=\"norm\", aggfunc=\"mean\").reset_index()\n    for c in [\"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"mid\",\"final\"]:\n        if c not in piv.columns: piv[c] = np.nan\n    piv[\"CA_qa\"] = piv[[\"quiz1\",\"quiz2\",\"quiz3\",\"assignment\"]].mean(axis=1, skipna=True).fillna(0)\n    piv[\"Mid_norm\"]   = piv[\"mid\"].fillna(0)\n    piv[\"Final_norm\"] = piv[\"final\"].fillna(0)\n    return piv\n\nwide = build_assessment_wide(assess)\n\n# Attendance merge\natt = engage.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False)[\"attendance_percentage\"].mean()\nwide = wide.merge(att, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\nwide[\"Att\"] = wide[\"attendance_percentage\"].fillna(0)/100.0\n\n# New Policy Final %\nwide[\"final_percentage_new\"] = (0.10*wide[\"Att\"] + 0.15*wide[\"CA_qa\"] +\n                                0.25*wide[\"Mid_norm\"] + 0.50*wide[\"Final_norm\"]) * 100\n\nwide[\"letter_grade_new\"] = wide[\"final_percentage_new\"].apply(letter_from_pct)\n\n# Save a new policy summary (optional)\nnew_policy = wide[[\"student_id\",\"course_code\",\"semester_no\",\"final_percentage_new\",\"letter_grade_new\"]].copy()\nnew_policy.to_csv(\"course_summary_grades_NEW_POLICY.csv\", index=False)\nprint(\"Saved course_summary_grades_NEW_POLICY.csv\", new_policy.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:04:07.548470Z","iopub.execute_input":"2025-10-07T05:04:07.549221Z","iopub.status.idle":"2025-10-07T05:04:07.616444Z","shell.execute_reply.started":"2025-10-07T05:04:07.549171Z","shell.execute_reply":"2025-10-07T05:04:07.615593Z"}},"outputs":[{"name":"stdout","text":"Saved course_summary_grades_NEW_POLICY.csv (4000, 5)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =========================================================\n# 3) Supervised Learning Tables (Regression & Risk)\n# =========================================================\ndef join_engagement(piv):\n    agg_e = engage.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n        \"attendance_percentage\":\"mean\",\n        \"lms_logins\":\"sum\",\"forum_posts\":\"sum\",\"chatbot_interactions\":\"sum\",\n        \"on_time_submission_rate\":\"mean\"\n    })\n    out = piv.merge(agg_e, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n    return add_course_meta(out)\n\ndef join_feedback(piv):\n    fb = feedback.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n        \"interest_rating\":\"mean\",\"perceived_difficulty\":\"mean\"\n    })\n    return piv.merge(fb, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n\ndef build_regression_table():\n    X = build_assessment_wide(assess)\n    X = join_engagement(X)\n    X = join_feedback(X)\n    y = new_policy.rename(columns={\n        \"final_percentage_new\":\"final_percentage\",\n        \"letter_grade_new\":\"letter_grade\"\n    })\n    df = X.merge(y, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"inner\")\n    # prior semester average\n    prior = (y.assign(prior_sem=lambda d: d[\"semester_no\"]-1)\n               .merge(y[[\"student_id\",\"semester_no\",\"final_percentage\"]]\n               .rename(columns={\"semester_no\":\"prior_sem\",\"final_percentage\":\"prior_pct\"}),\n               on=[\"student_id\",\"prior_sem\"], how=\"left\"))\n    prior_gpa = (prior.groupby([\"student_id\",\"semester_no\"], as_index=False)[\"prior_pct\"]\n                      .mean().rename(columns={\"prior_pct\":\"prior_sem_avg\"}))\n    df = df.merge(prior_gpa, on=[\"student_id\",\"semester_no\"], how=\"left\")\n    return df.fillna(0)\n\nreg = build_regression_table()\nreg.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:04:12.518570Z","iopub.execute_input":"2025-10-07T05:04:12.519077Z","iopub.status.idle":"2025-10-07T05:04:12.592614Z","shell.execute_reply.started":"2025-10-07T05:04:12.519047Z","shell.execute_reply":"2025-10-07T05:04:12.591872Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  student_id course_code  semester_no  assignment   final     mid   quiz1  \\\n0  CSE240001    CSE 4301            3     0.75000 0.68333 0.77500 0.70000   \n1  CSE240001    CSE 4302            3     0.70000 0.78333 0.72500 0.80000   \n2  CSE240001    CSE 4303            3     0.70000 0.71667 0.75000 0.80000   \n\n    quiz2   quiz3   CA_qa  ...  forum_posts  chatbot_interactions  \\\n0 0.70000 0.90000 0.76250  ...            6                     5   \n1 0.80000 0.80000 0.77500  ...            2                     3   \n2 0.70000 0.80000 0.75000  ...            3                     8   \n\n   on_time_submission_rate  course_type  credit_hours  interest_rating  \\\n0                  0.80000       Theory       3.00000          2.00000   \n1                  0.96000          Lab       1.50000          2.00000   \n2                  0.77000       Theory       3.00000          2.00000   \n\n   perceived_difficulty final_percentage  letter_grade  prior_sem_avg  \n0               3.00000         73.80517            A-        0.00000  \n1               2.00000         77.26867             A        0.00000  \n2               4.00000         74.64133            A-        0.00000  \n\n[3 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>course_code</th>\n      <th>semester_no</th>\n      <th>assignment</th>\n      <th>final</th>\n      <th>mid</th>\n      <th>quiz1</th>\n      <th>quiz2</th>\n      <th>quiz3</th>\n      <th>CA_qa</th>\n      <th>...</th>\n      <th>forum_posts</th>\n      <th>chatbot_interactions</th>\n      <th>on_time_submission_rate</th>\n      <th>course_type</th>\n      <th>credit_hours</th>\n      <th>interest_rating</th>\n      <th>perceived_difficulty</th>\n      <th>final_percentage</th>\n      <th>letter_grade</th>\n      <th>prior_sem_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CSE240001</td>\n      <td>CSE 4301</td>\n      <td>3</td>\n      <td>0.75000</td>\n      <td>0.68333</td>\n      <td>0.77500</td>\n      <td>0.70000</td>\n      <td>0.70000</td>\n      <td>0.90000</td>\n      <td>0.76250</td>\n      <td>...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0.80000</td>\n      <td>Theory</td>\n      <td>3.00000</td>\n      <td>2.00000</td>\n      <td>3.00000</td>\n      <td>73.80517</td>\n      <td>A-</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CSE240001</td>\n      <td>CSE 4302</td>\n      <td>3</td>\n      <td>0.70000</td>\n      <td>0.78333</td>\n      <td>0.72500</td>\n      <td>0.80000</td>\n      <td>0.80000</td>\n      <td>0.80000</td>\n      <td>0.77500</td>\n      <td>...</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.96000</td>\n      <td>Lab</td>\n      <td>1.50000</td>\n      <td>2.00000</td>\n      <td>2.00000</td>\n      <td>77.26867</td>\n      <td>A</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CSE240001</td>\n      <td>CSE 4303</td>\n      <td>3</td>\n      <td>0.70000</td>\n      <td>0.71667</td>\n      <td>0.75000</td>\n      <td>0.80000</td>\n      <td>0.70000</td>\n      <td>0.80000</td>\n      <td>0.75000</td>\n      <td>...</td>\n      <td>3</td>\n      <td>8</td>\n      <td>0.77000</td>\n      <td>Theory</td>\n      <td>3.00000</td>\n      <td>2.00000</td>\n      <td>4.00000</td>\n      <td>74.64133</td>\n      <td>A-</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# =========================================================\n# 3) Supervised Learning Tables (Regression & Risk)\n# =========================================================\ndef join_engagement(piv):\n    agg_e = engage.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n        \"attendance_percentage\":\"mean\",\n        \"lms_logins\":\"sum\",\"forum_posts\":\"sum\",\"chatbot_interactions\":\"sum\",\n        \"on_time_submission_rate\":\"mean\"\n    })\n    out = piv.merge(agg_e, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n    return add_course_meta(out)\n\ndef join_feedback(piv):\n    fb = feedback.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n        \"interest_rating\":\"mean\",\"perceived_difficulty\":\"mean\"\n    })\n    return piv.merge(fb, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n\ndef build_regression_table():\n    X = build_assessment_wide(assess)\n    X = join_engagement(X)\n    X = join_feedback(X)\n    y = new_policy.rename(columns={\n        \"final_percentage_new\":\"final_percentage\",\n        \"letter_grade_new\":\"letter_grade\"\n    })\n    df = X.merge(y, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"inner\")\n    # prior semester average\n    prior = (y.assign(prior_sem=lambda d: d[\"semester_no\"]-1)\n               .merge(y[[\"student_id\",\"semester_no\",\"final_percentage\"]]\n               .rename(columns={\"semester_no\":\"prior_sem\",\"final_percentage\":\"prior_pct\"}),\n               on=[\"student_id\",\"prior_sem\"], how=\"left\"))\n    prior_gpa = (prior.groupby([\"student_id\",\"semester_no\"], as_index=False)[\"prior_pct\"]\n                      .mean().rename(columns={\"prior_pct\":\"prior_sem_avg\"}))\n    df = df.merge(prior_gpa, on=[\"student_id\",\"semester_no\"], how=\"left\")\n    return df.fillna(0)\n\nreg = build_regression_table()\nreg.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:04:17.017881Z","iopub.execute_input":"2025-10-07T05:04:17.018163Z","iopub.status.idle":"2025-10-07T05:04:17.085226Z","shell.execute_reply.started":"2025-10-07T05:04:17.018142Z","shell.execute_reply":"2025-10-07T05:04:17.084552Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  student_id course_code  semester_no  assignment   final     mid   quiz1  \\\n0  CSE240001    CSE 4301            3     0.75000 0.68333 0.77500 0.70000   \n1  CSE240001    CSE 4302            3     0.70000 0.78333 0.72500 0.80000   \n2  CSE240001    CSE 4303            3     0.70000 0.71667 0.75000 0.80000   \n\n    quiz2   quiz3   CA_qa  ...  forum_posts  chatbot_interactions  \\\n0 0.70000 0.90000 0.76250  ...            6                     5   \n1 0.80000 0.80000 0.77500  ...            2                     3   \n2 0.70000 0.80000 0.75000  ...            3                     8   \n\n   on_time_submission_rate  course_type  credit_hours  interest_rating  \\\n0                  0.80000       Theory       3.00000          2.00000   \n1                  0.96000          Lab       1.50000          2.00000   \n2                  0.77000       Theory       3.00000          2.00000   \n\n   perceived_difficulty final_percentage  letter_grade  prior_sem_avg  \n0               3.00000         73.80517            A-        0.00000  \n1               2.00000         77.26867             A        0.00000  \n2               4.00000         74.64133            A-        0.00000  \n\n[3 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>course_code</th>\n      <th>semester_no</th>\n      <th>assignment</th>\n      <th>final</th>\n      <th>mid</th>\n      <th>quiz1</th>\n      <th>quiz2</th>\n      <th>quiz3</th>\n      <th>CA_qa</th>\n      <th>...</th>\n      <th>forum_posts</th>\n      <th>chatbot_interactions</th>\n      <th>on_time_submission_rate</th>\n      <th>course_type</th>\n      <th>credit_hours</th>\n      <th>interest_rating</th>\n      <th>perceived_difficulty</th>\n      <th>final_percentage</th>\n      <th>letter_grade</th>\n      <th>prior_sem_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CSE240001</td>\n      <td>CSE 4301</td>\n      <td>3</td>\n      <td>0.75000</td>\n      <td>0.68333</td>\n      <td>0.77500</td>\n      <td>0.70000</td>\n      <td>0.70000</td>\n      <td>0.90000</td>\n      <td>0.76250</td>\n      <td>...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0.80000</td>\n      <td>Theory</td>\n      <td>3.00000</td>\n      <td>2.00000</td>\n      <td>3.00000</td>\n      <td>73.80517</td>\n      <td>A-</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CSE240001</td>\n      <td>CSE 4302</td>\n      <td>3</td>\n      <td>0.70000</td>\n      <td>0.78333</td>\n      <td>0.72500</td>\n      <td>0.80000</td>\n      <td>0.80000</td>\n      <td>0.80000</td>\n      <td>0.77500</td>\n      <td>...</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.96000</td>\n      <td>Lab</td>\n      <td>1.50000</td>\n      <td>2.00000</td>\n      <td>2.00000</td>\n      <td>77.26867</td>\n      <td>A</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CSE240001</td>\n      <td>CSE 4303</td>\n      <td>3</td>\n      <td>0.70000</td>\n      <td>0.71667</td>\n      <td>0.75000</td>\n      <td>0.80000</td>\n      <td>0.70000</td>\n      <td>0.80000</td>\n      <td>0.75000</td>\n      <td>...</td>\n      <td>3</td>\n      <td>8</td>\n      <td>0.77000</td>\n      <td>Theory</td>\n      <td>3.00000</td>\n      <td>2.00000</td>\n      <td>4.00000</td>\n      <td>74.64133</td>\n      <td>A-</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 24 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# ==== Final-Grade Regressor (LightGBM) — fixed for no 'verbose' kw ====\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport lightgbm as lgb\nimport numpy as np\n\nFEATURES = [\n    \"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"CA_qa\",\"Mid_norm\",\"Final_norm\",\n    \"attendance_percentage\",\"lms_logins\",\"forum_posts\",\"chatbot_interactions\",\"on_time_submission_rate\",\n    \"interest_rating\",\"perceived_difficulty\",\"credit_hours\",\"prior_sem_avg\"\n]\nCAT = [\"course_type\",\"course_code\"]\n\n# Ensure categorical dtypes for LightGBM\nfor c in CAT:\n    reg[c] = reg[c].astype(\"category\")\n\nX = reg[FEATURES+CAT]\ny = reg[\"final_percentage\"]\ngroups = reg[\"semester_no\"]\n\ngkf = GroupKFold(n_splits=2)\noof, models = np.zeros(len(reg)), []\n\nfor tr, va in gkf.split(X, y, groups=groups):\n    model = lgb.LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.03,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        random_state=42\n    )\n    model.fit(\n        X.iloc[tr], y.iloc[tr],\n        eval_set=[(X.iloc[va], y.iloc[va])],\n        eval_metric=\"l2\",\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100),\n            lgb.log_evaluation(period=50)  # prints every 50 iters; remove if you want silence\n        ]\n    )\n    pred = model.predict(X.iloc[va])\n    oof[va] = pred\n    models.append(model)\n\nprint(\"Regressor MAE:\", mean_absolute_error(y, oof))\nprint(\"Regressor RMSE:\", mean_squared_error(y, oof, squared=False))\n\ngrade_model = models[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:06:09.605709Z","iopub.execute_input":"2025-10-07T05:06:09.606464Z","iopub.status.idle":"2025-10-07T05:06:11.490770Z","shell.execute_reply.started":"2025-10-07T05:06:09.606435Z","shell.execute_reply":"2025-10-07T05:06:11.490062Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000184 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 559\n[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 17\n[LightGBM] [Info] Start training from score 71.892533\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid_0's l2: 5.6584\n[100]\tvalid_0's l2: 0.536223\n[150]\tvalid_0's l2: 0.174371\n[200]\tvalid_0's l2: 0.137392\n[250]\tvalid_0's l2: 0.129542\n[300]\tvalid_0's l2: 0.125384\n[350]\tvalid_0's l2: 0.121882\n[400]\tvalid_0's l2: 0.120283\n[450]\tvalid_0's l2: 0.118828\n[500]\tvalid_0's l2: 0.117819\n[550]\tvalid_0's l2: 0.116915\n[600]\tvalid_0's l2: 0.115981\n[650]\tvalid_0's l2: 0.115018\n[700]\tvalid_0's l2: 0.114079\n[750]\tvalid_0's l2: 0.113735\n[800]\tvalid_0's l2: 0.112956\n[850]\tvalid_0's l2: 0.112747\n[900]\tvalid_0's l2: 0.112446\n[950]\tvalid_0's l2: 0.112352\n[1000]\tvalid_0's l2: 0.111983\n[1050]\tvalid_0's l2: 0.111827\n[1100]\tvalid_0's l2: 0.111663\n[1150]\tvalid_0's l2: 0.111594\n[1200]\tvalid_0's l2: 0.111317\nDid not meet early stopping. Best iteration is:\n[1200]\tvalid_0's l2: 0.111317\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 762\n[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 18\n[LightGBM] [Info] Start training from score 72.639136\nTraining until validation scores don't improve for 100 rounds\n[50]\tvalid_0's l2: 6.03457\n[100]\tvalid_0's l2: 0.745357\n[150]\tvalid_0's l2: 0.318565\n[200]\tvalid_0's l2: 0.264214\n[250]\tvalid_0's l2: 0.248834\n[300]\tvalid_0's l2: 0.235926\n[350]\tvalid_0's l2: 0.227487\n[400]\tvalid_0's l2: 0.225146\n[450]\tvalid_0's l2: 0.228759\nEarly stopping, best iteration is:\n[387]\tvalid_0's l2: 0.224059\nRegressor MAE: 0.25402312867886917\nRegressor RMSE: 0.4094970538081607\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# =========================================================\n# 5) Early Risk Classifier (after-mid checkpoint)\n# =========================================================\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\n# Use features available after midterm (exclude final_norm)\nAFTER_MID = [\n    \"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"CA_qa\",\"Mid_norm\",\n    \"attendance_percentage\",\"lms_logins\",\"forum_posts\",\"chatbot_interactions\",\"on_time_submission_rate\",\n    \"interest_rating\",\"perceived_difficulty\",\"credit_hours\",\"course_type\",\"course_code\"\n]\nrisk_df = reg.copy()\ntarget_threshold = 60.0\nrisk_df[\"risk_label\"] = (risk_df[\"final_percentage\"] < target_threshold).astype(int)\n\nXr = risk_df[AFTER_MID]\nXr_enc = pd.get_dummies(Xr, columns=[\"course_type\",\"course_code\"], drop_first=True)\nyr = risk_df[\"risk_label\"]\n\nrisk_model = LogisticRegression(max_iter=500)\nrisk_model.fit(Xr_enc, yr)\nprob = risk_model.predict_proba(Xr_enc)[:,1]\nprint(\"Risk ROC-AUC:\", roc_auc_score(yr, prob))\nprint(\"Risk PR-AUC:\", average_precision_score(yr, prob))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:06:15.735139Z","iopub.execute_input":"2025-10-07T05:06:15.735472Z","iopub.status.idle":"2025-10-07T05:06:18.049933Z","shell.execute_reply.started":"2025-10-07T05:06:15.735451Z","shell.execute_reply":"2025-10-07T05:06:18.049225Z"}},"outputs":[{"name":"stdout","text":"Risk ROC-AUC: 0.9782412810021123\nRisk PR-AUC: 0.8599434930601237\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==== Topic Mastery with BKT (robust to pyBKT variants) ====\n# Produces P_mastery with columns: user_id, skill_name, p_mastery (latest per topic)\n\nfrom pyBKT.models import Model\nimport pandas as pd\nimport numpy as np\n\n# 1) Build per-attempt sequences\nseq = assess.copy()\nseq[\"correct\"] = (seq[\"raw_score\"] / seq[\"max_score\"] >= 0.6).astype(int)\nseq = (seq.merge(topics, on=[\"course_code\",\"topic_id\"], how=\"left\")\n          .dropna(subset=[\"topic_name\"]))\n\nseq_bkt = (seq.rename(columns={\"student_id\":\"user_id\", \"topic_name\":\"skill_name\"})\n              [[\"user_id\",\"skill_name\",\"correct\",\"timestamp\"]]\n              .sort_values([\"user_id\",\"skill_name\",\"timestamp\"])\n              .reset_index(drop=True))\nseq_bkt[\"user_id\"]    = seq_bkt[\"user_id\"].astype(str)\nseq_bkt[\"skill_name\"] = seq_bkt[\"skill_name\"].astype(str)\nseq_bkt[\"seq_idx\"]    = seq_bkt.groupby([\"user_id\",\"skill_name\"]).cumcount()\n\nprint(f\"Unique skills: {seq_bkt['skill_name'].nunique()} | Rows: {len(seq_bkt)}\")\n\n# 2) Fit pyBKT across all skills\nbkt = Model(seed=42, num_fits=5)\nbkt.fit(data=seq_bkt[[\"user_id\",\"skill_name\",\"correct\",\"timestamp\"]])\n\n# 3) Predict mastery per interaction\nmastery = bkt.predict(data=seq_bkt[[\"user_id\",\"skill_name\",\"correct\",\"timestamp\"]]).reset_index(drop=True)\n\n# Attach alignment columns\nmastery[\"user_id\"]    = seq_bkt[\"user_id\"].values\nmastery[\"skill_name\"] = seq_bkt[\"skill_name\"].values\nmastery[\"seq_idx\"]    = seq_bkt[\"seq_idx\"].values\n\n# 4) Extract p_mastery from whatever the model returned\n#    - Preferred: 'posterior' (some builds)\n#    - Common alt: 'state_predictions' (prob vector over states at this step)\n#    - Fallback:    use 'correct_predictions' as a proxy (less ideal)\n\npcol = None\nfor cand in mastery.columns:\n    if cand.lower().startswith(\"posterior\"):\n        pcol = cand\n        break\n\nif pcol is None and \"state_predictions\" in mastery.columns:\n    def to_known_prob(v):\n        # v is a list/array of state probabilities for this timestep\n        a = np.array(v, dtype=float).ravel()\n        # pyBKT often orders states as [unlearned, learned]; take index 1 if size==2.\n        if a.size >= 2:\n            return float(a[1])   # assume index 1 = \"Known\"\n        # if a single value or unexpected shape, use max as a conservative proxy\n        return float(a.max()) if a.size else np.nan\n    mastery[\"p_mastery\"] = mastery[\"state_predictions\"].apply(to_known_prob)\nelse:\n    if pcol is not None:\n        mastery[\"p_mastery\"] = mastery[pcol].astype(float)\n    elif \"correct_predictions\" in mastery.columns:\n        # Fallback proxy: probability of a correct response (not pure mastery, but usable)\n        mastery[\"p_mastery\"] = mastery[\"correct_predictions\"].astype(float)\n    else:\n        raise KeyError(f\"Could not locate posterior/state columns. Columns: {list(mastery.columns)}\")\n\n# 5) Keep latest attempt per (user, skill)\nlast_idx = mastery.groupby([\"user_id\",\"skill_name\"])[\"seq_idx\"].transform(\"max\")\nlatest = mastery[mastery[\"seq_idx\"] == last_idx].copy()\n\nP_mastery = latest[[\"user_id\",\"skill_name\",\"p_mastery\"]].reset_index(drop=True)\nprint(\"Mastery table shape:\", P_mastery.shape)\ndisplay(P_mastery.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:10:32.672289Z","iopub.execute_input":"2025-10-07T05:10:32.672935Z","iopub.status.idle":"2025-10-07T05:10:33.841739Z","shell.execute_reply.started":"2025-10-07T05:10:32.672915Z","shell.execute_reply":"2025-10-07T05:10:33.840980Z"}},"outputs":[{"name":"stdout","text":"Unique skills: 119 | Rows: 24000\nMastery table shape: (14983, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     user_id                     skill_name  p_mastery\n0  CSE240001   Algorithm Implementation Lab    0.93610\n1  CSE240001              Arrays & Pointers    0.42075\n2  CSE240001            Asymptotic Analysis    0.99993\n3  CSE240001            BJT Characteristics    0.88224\n4  CSE240001                 CLT & Sampling    0.98180\n5  CSE240001   CPU Organization & Registers    0.76022\n6  CSE240001  Cellular Basics & Propagation    0.97710\n7  CSE240001              Classes & Objects    0.86788\n8  CSE240001         Complexity Experiments    0.93703\n9  CSE240001         Concurrency & Recovery    0.89161","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>skill_name</th>\n      <th>p_mastery</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CSE240001</td>\n      <td>Algorithm Implementation Lab</td>\n      <td>0.93610</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CSE240001</td>\n      <td>Arrays &amp; Pointers</td>\n      <td>0.42075</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CSE240001</td>\n      <td>Asymptotic Analysis</td>\n      <td>0.99993</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CSE240001</td>\n      <td>BJT Characteristics</td>\n      <td>0.88224</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CSE240001</td>\n      <td>CLT &amp; Sampling</td>\n      <td>0.98180</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CSE240001</td>\n      <td>CPU Organization &amp; Registers</td>\n      <td>0.76022</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CSE240001</td>\n      <td>Cellular Basics &amp; Propagation</td>\n      <td>0.97710</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CSE240001</td>\n      <td>Classes &amp; Objects</td>\n      <td>0.86788</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>CSE240001</td>\n      <td>Complexity Experiments</td>\n      <td>0.93703</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CSE240001</td>\n      <td>Concurrency &amp; Recovery</td>\n      <td>0.89161</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# =========================================================\n# 7) Elective Recommender (Hybrid: content + difficulty [+ perf])\n# =========================================================\nfrom sentence_transformers import SentenceTransformer\n\ncourse_topics = (topics.groupby(\"course_code\")[\"topic_name\"]\n                        .apply(lambda x: \"; \".join(map(str,x))).reset_index())\ncourse_text = courses.merge(course_topics, on=\"course_code\", how=\"left\")\ncourse_text[\"desc\"] = course_text[\"course_title\"].fillna(\"\") + \" | \" + course_text[\"topic_name\"].fillna(\"\")\n\nembedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\ncourse_vecs = embedder.encode(course_text[\"desc\"].tolist(), convert_to_numpy=True, normalize_embeddings=True)\n\ndef student_profile_embedding(student_id: str):\n    fb = feedback[feedback[\"student_id\"]==student_id].copy()\n    if fb.empty:\n        return course_vecs.mean(axis=0)\n    fb = fb.merge(course_text[[\"course_code\",\"desc\"]], on=\"course_code\", how=\"left\")\n    fb[\"w_interest\"] = fb[\"interest_rating\"].clip(1,5)/5.0\n\n    cur = reg[reg[\"student_id\"]==student_id].copy()\n    if cur.empty:\n        fb[\"w_perf\"] = 0.5\n    else:\n        pred = grade_model.predict(cur[FEATURES+CAT])\n        cur = cur.assign(pred_final=pred)[[\"course_code\",\"pred_final\"]]\n        fb = fb.merge(cur, on=\"course_code\", how=\"left\")\n        fb[\"w_perf\"] = fb[\"pred_final\"].fillna(fb[\"pred_final\"].mean() if not np.isnan(fb[\"pred_final\"].mean()) else 60)/100.0\n\n    fb[\"w\"] = 0.6*fb[\"w_interest\"] + 0.4*fb[\"w_perf\"]\n    vecs = embedder.encode(fb[\"desc\"].fillna(\"\").tolist(), convert_to_numpy=True, normalize_embeddings=True)\n    w = fb[\"w\"].values.reshape(-1,1)\n    return (vecs*w).sum(axis=0)/max(1e-6,w.sum())\n\ndef recommend_courses(student_id: str, topk=5, exclude_current=True):\n    prof = student_profile_embedding(student_id)\n    sims = (course_vecs @ prof)\n    df = course_text.copy()\n    cur = reg[reg[\"student_id\"]==student_id][[\"course_code\"]].drop_duplicates()\n    if exclude_current:\n        df = df[~df[\"course_code\"].isin(cur[\"course_code\"])]\n    fb = feedback[feedback[\"student_id\"]==student_id][[\"course_code\",\"perceived_difficulty\"]]\n    df = df.merge(fb, on=\"course_code\", how=\"left\")\n    df[\"diff_penalty\"] = 1 - (df[\"perceived_difficulty\"].fillna(3)/5.0)\n    df[\"score\"] = 0.7*sims[df.index] + 0.3*df[\"diff_penalty\"]\n    return df.sort_values(\"score\", ascending=False).head(topk)[\n        [\"course_code\",\"course_title\",\"semester_no\",\"score\"]\n    ]\n\nsample_student = students[\"student_id\"].iloc[0]\nrecommend_courses(sample_student, topk=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:10:37.083928Z","iopub.execute_input":"2025-10-07T05:10:37.084651Z","iopub.status.idle":"2025-10-07T05:10:39.171640Z","shell.execute_reply.started":"2025-10-07T05:10:37.084624Z","shell.execute_reply":"2025-10-07T05:10:39.170740Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78c2fdb88d07434f80735b9a30ea45f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0c244da2634dc08c2bbd479bbf72d1"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [course_code, course_title, semester_no, score]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>course_code</th>\n      <th>course_title</th>\n      <th>semester_no</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# =========================================================\n# 8) Learning Path Generator (Jinja2 template)\n# =========================================================\nfrom jinja2 import Template\n\nTPL = Template(\"\"\"\n# Personalized Plan for {{ name }} — Sem {{ sem }}\n**Attendance**: {{ attendance }}% {% if attendance < 85 %}(⚠ target ≥ 85%){% endif %}\n\n**Predicted final**: {{ pred_final|round(1) }}% ({{ letter }})  \n**Risk (after-mid)**: {{ (risk_prob*100)|round(1) }}%\n\n## Focus Topics (next 2 weeks)\n{% for t in weak_topics %}\n- **{{ t.skill }}** — mastery {{ (t.p*100)|round(0) }}% → 2× 45min practice + 1× timed quiz\n{% endfor %}\n\n## Upcoming Topics\n{% for u in upcoming %}\n- **{{ u }}** — pre-read + short quiz before class\n{% endfor %}\n\n## Routine\n- Mon/Wed/Fri: 1h weak-topic drills; Tue/Thu: 30m recap + spaced repetition\n- Weekend: 1 mock (final-style) on weak topics\n\n## Electives Next Term\n{% for r in recs %}\n- **{{ r.course_code }} — {{ r.course_title }}** (fit score {{ r.score|round(2) }})\n{% endfor %}\n\"\"\".strip())\n\n# Helper inference encoders\nAFTER_MID_CAT = [c for c in Xr_enc.columns if c not in Xr.columns]\n\ndef generate_learning_path(student_id: str, course_code: str):\n    row = reg[(reg[\"student_id\"]==student_id)&(reg[\"course_code\"]==course_code)].tail(1)\n    if row.empty:\n        return f\"# No data for {student_id} {course_code}\"\n    r = row.iloc[0]\n\n    # predicted final\n    pred = float(grade_model.predict(row[FEATURES+CAT])[0])\n    # risk probability\n    xr = row[[c for c in AFTER_MID if c in row.columns]].copy()\n    xr_enc = pd.get_dummies(xr, columns=[\"course_type\",\"course_code\"], drop_first=True)\n    for m in set(Xr_enc.columns)-set(xr_enc.columns):\n        xr_enc[m]=0\n    xr_enc = xr_enc[Xr_enc.columns]\n    risk_prob = float(risk_model.predict_proba(xr_enc)[0,1])\n\n    attendance = float(r[\"attendance_percentage\"])\n    letter = letter_from_pct(pred)\n\n    # weak topics (BKT)\n    pm = P_mastery[P_mastery[\"user_id\"]==student_id].copy()\n    c_topics = topics[topics[\"course_code\"]==course_code][\"topic_name\"].unique().tolist()\n    pm = pm[pm[\"skill_name\"].isin(c_topics)]\n    weak = pm.sort_values(\"p_mastery\").head(3)\n    weak_topics = [{\"skill\": s, \"p\": float(p)} for s,p in zip(weak[\"skill_name\"], weak[\"p_mastery\"])]\n\n    seen = (assess[(assess[\"student_id\"]==student_id)&(assess[\"course_code\"]==course_code)]\n              .merge(topics, on=[\"course_code\",\"topic_id\"], how=\"left\")[\"topic_name\"]\n              .dropna().unique().tolist())\n    all_topics = topics[topics[\"course_code\"]==course_code][\"topic_name\"].tolist()\n    upcoming = [t for t in all_topics if t not in seen][:3]\n\n    recs = recommend_courses(student_id, topk=3).to_dict(\"records\")\n    name = students[students[\"student_id\"]==student_id][\"name\"].iloc[0]\n    sem  = int(r[\"semester_no\"])\n\n    return TPL.render(\n        name=name, sem=sem, attendance=attendance,\n        pred_final=pred, letter=letter, risk_prob=risk_prob,\n        weak_topics=weak_topics, upcoming=upcoming, recs=recs\n    )\n\n# Demo (first student & first course)\ndemo_student = students[\"student_id\"].iloc[0]\ndemo_course  = courses[\"course_code\"].iloc[0]\nprint(generate_learning_path(demo_student, demo_course))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:10:57.028467Z","iopub.execute_input":"2025-10-07T05:10:57.029208Z","iopub.status.idle":"2025-10-07T05:10:57.111354Z","shell.execute_reply.started":"2025-10-07T05:10:57.029166Z","shell.execute_reply":"2025-10-07T05:10:57.110458Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90dc4d65ee29474a8d8e375333e6b1d2"}},"metadata":{}},{"name":"stdout","text":"# Personalized Plan for Sami Rahman — Sem 3\n**Attendance**: 91.14% \n\n**Predicted final**: 81.1% (A+)  \n**Risk (after-mid)**: 0.0%\n\n## Focus Topics (next 2 weeks)\n\n- **Eigenvalues & Eigenvectors** — mastery 72.0% → 2× 45min practice + 1× timed quiz\n\n- **Diagonalization** — mastery 82.0% → 2× 45min practice + 1× timed quiz\n\n- **Gaussian Elimination & LU** — mastery 84.0% → 2× 45min practice + 1× timed quiz\n\n\n## Upcoming Topics\n\n- **Vector Spaces & Bases** — pre-read + short quiz before class\n\n- **Gram-Schmidt & QR** — pre-read + short quiz before class\n\n- **Determinants & Cofactors** — pre-read + short quiz before class\n\n\n## Routine\n- Mon/Wed/Fri: 1h weak-topic drills; Tue/Thu: 30m recap + spaced repetition\n- Weekend: 1 mock (final-style) on weak topics\n\n## Electives Next Term\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ==== Export current state & learning-path snapshot for ALL students/courses ====\n# Output: current_state_<YYYY-MM-DD>.csv  (one row per student×course)\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# --------- Helpers (use existing objects if present; otherwise build) ----------\ndef ensure_regression_table():\n    global reg\n    try:\n        assert isinstance(reg, pd.DataFrame) and \"final_percentage\" in reg.columns\n        return reg\n    except Exception:\n        # Rebuild regression table using functions from earlier cells\n        def build_assessment_wide(df):\n            piv = df.pivot_table(index=[\"student_id\",\"course_code\",\"semester_no\"],\n                                 columns=\"assessment_type\", values=\"norm\", aggfunc=\"mean\").reset_index()\n            for c in [\"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"mid\",\"final\"]:\n                if c not in piv.columns: piv[c] = np.nan\n            piv[\"CA_qa\"] = piv[[\"quiz1\",\"quiz2\",\"quiz3\",\"assignment\"]].mean(axis=1, skipna=True).fillna(0)\n            piv[\"Mid_norm\"]   = piv[\"mid\"].fillna(0)\n            piv[\"Final_norm\"] = piv[\"final\"].fillna(0)\n            return piv\n\n        def add_course_meta(df):\n            return df.merge(courses[[\"course_code\",\"semester_no\",\"course_type\",\"credit_hours\"]].drop_duplicates(),\n                            on=[\"course_code\",\"semester_no\"], how=\"left\")\n\n        def join_engagement(piv):\n            agg_e = engage.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n                \"attendance_percentage\":\"mean\",\n                \"lms_logins\":\"sum\",\"forum_posts\":\"sum\",\"chatbot_interactions\":\"sum\",\n                \"on_time_submission_rate\":\"mean\"\n            })\n            out = piv.merge(agg_e, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n            return add_course_meta(out)\n\n        def join_feedback(piv):\n            fb = feedback.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n                \"interest_rating\":\"mean\",\"perceived_difficulty\":\"mean\"\n            })\n            return piv.merge(fb, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n\n        # recompute new-policy final %\n        if \"norm\" not in assess.columns:\n            assess[\"norm\"] = (assess[\"raw_score\"]/assess[\"max_score\"]).clip(0,1)\n\n        wide = build_assessment_wide(assess)\n        att = engage.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False)[\"attendance_percentage\"].mean()\n        wide = wide.merge(att, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n        wide[\"Att\"] = wide[\"attendance_percentage\"].fillna(0)/100.0\n        new_policy = wide[[\"student_id\",\"course_code\",\"semester_no\",\"CA_qa\",\"Mid_norm\",\"Final_norm\",\"Att\"]].copy()\n        new_policy[\"final_percentage\"] = (0.10*new_policy[\"Att\"] + 0.15*new_policy[\"CA_qa\"]\n                                          + 0.25*new_policy[\"Mid_norm\"] + 0.50*new_policy[\"Final_norm\"]) * 100.0\n\n        X = build_assessment_wide(assess)\n        X = join_engagement(X)\n        X = join_feedback(X)\n        df = X.merge(new_policy[[\"student_id\",\"course_code\",\"semester_no\",\"final_percentage\"]],\n                     on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n\n        # prior semester avg\n        prior = (new_policy.assign(prior_sem=lambda d: d[\"semester_no\"]-1)\n                 .merge(new_policy[[\"student_id\",\"semester_no\",\"final_percentage\"]]\n                        .rename(columns={\"semester_no\":\"prior_sem\",\"final_percentage\":\"prior_pct\"}),\n                        on=[\"student_id\",\"prior_sem\"], how=\"left\"))\n        prior_gpa = (prior.groupby([\"student_id\",\"semester_no\"], as_index=False)[\"prior_pct\"]\n                          .mean().rename(columns={\"prior_pct\":\"prior_sem_avg\"}))\n        df = df.merge(prior_gpa, on=[\"student_id\",\"semester_no\"], how=\"left\").fillna(0)\n        return df\n\ndef ensure_letter_from_pct():\n    try:\n        letter_from_pct  # exists\n        return letter_from_pct\n    except NameError:\n        def _letter_from_pct(p):\n            return (\"A+\" if p>=80 else \"A\" if p>=75 else \"A-\" if p>=70 else\n                    \"B+\" if p>=65 else \"B\" if p>=60 else \"B-\" if p>=55 else\n                    \"C+\" if p>=50 else \"C\" if p>=45 else \"D\" if p>=40 else \"F\")\n        return _letter_from_pct\n\ndef ensure_P_mastery():\n    global P_mastery\n    try:\n        assert isinstance(P_mastery, pd.DataFrame) and {\"user_id\",\"skill_name\",\"p_mastery\"} <= set(P_mastery.columns)\n        return P_mastery\n    except Exception:\n        # build a lightweight proxy: use correctness rate per topic as mastery if pyBKT not available\n        tmp = (assess.merge(topics, on=[\"course_code\",\"topic_id\"], how=\"left\")\n                    .dropna(subset=[\"topic_name\"]))\n        tmp[\"correct\"] = (tmp[\"raw_score\"]/tmp[\"max_score\"] >= 0.6).astype(int)\n        P = (tmp.groupby([\"student_id\",\"topic_name\"], as_index=False)[\"correct\"]\n                .mean().rename(columns={\"student_id\":\"user_id\",\"topic_name\":\"skill_name\",\"correct\":\"p_mastery\"}))\n        return P\n\ndef ensure_models_ready():\n    # returns callable predictors; if models missing, use simple heuristics\n    def _grade_predict(rows):\n        try:\n            _ = grade_model\n            return grade_model.predict(rows)\n        except Exception:\n            # heuristic = linear combo of mid/final/CA/attendance (same as label formula)\n            return (0.10*rows[\"attendance_percentage\"].fillna(0)/100.0 +\n                    0.15*rows[\"CA_qa\"].fillna(0) +\n                    0.25*rows[\"Mid_norm\"].fillna(0) +\n                    0.50*rows[\"Final_norm\"].fillna(0)) * 100.0\n\n    def _risk_predict(rows_enc):\n        try:\n            _ = risk_model\n            return risk_model.predict_proba(rows_enc)[:,1]\n        except Exception:\n            # heuristic: risk if current blended < 60%\n            approx = (0.10*rows_enc.get(\"attendance_percentage\",0)/100.0 +\n                      0.15*rows_enc.get(\"CA_qa\",0) +\n                      0.25*rows_enc.get(\"Mid_norm\",0)) * 100.0\n            return (approx < 60).astype(float)\n    return _grade_predict, _risk_predict\n\ndef ensure_recommender():\n    try:\n        recommend_courses  # exists\n        return recommend_courses\n    except NameError:\n        # simple content-based using topic overlap count\n        def _rec(student_id, topk=3, exclude_current=True):\n            st_courses = reg[reg[\"student_id\"]==student_id][\"course_code\"].unique().tolist()\n            course_topics = topics.groupby(\"course_code\")[\"topic_name\"].apply(set)\n            # pick topk by number of topics not yet taken\n            scores = []\n            taken_topics = set()\n            for c in st_courses:\n                taken_topics |= course_topics.get(c, set())\n            for c in courses[\"course_code\"].unique():\n                if exclude_current and c in st_courses: \n                    continue\n                s = len(course_topics.get(c,set()) - taken_topics)\n                title = courses[courses[\"course_code\"]==c][\"course_title\"].iloc[0]\n                sem   = courses[courses[\"course_code\"]==c][\"semester_no\"].iloc[0]\n                scores.append({\"course_code\":c, \"course_title\":title, \"semester_no\":sem, \"score\":float(s)})\n            return pd.DataFrame(scores).sort_values(\"score\", ascending=False).head(topk)\n        return _rec\n\n# Build or reuse artifacts\nreg = ensure_regression_table()\nletter_from_pct = ensure_letter_from_pct()\nP_mastery = ensure_P_mastery()\ngrade_predict, risk_predict = ensure_models_ready()\nrecommend_courses = ensure_recommender()\n\n# Columns we’ll export\nexport_rows = []\nrun_date = datetime.utcnow().date().isoformat()\n\n# Prepare encoder for risk (one-hot categories) if we trained one\ntry:\n    Xr_enc  # from training cell\n    risk_encoder_cols = list(Xr_enc.columns)\nexcept Exception:\n    risk_encoder_cols = None\n\n# For each student × their courses\nmerged_names = students[[\"student_id\",\"name\"]]\nreg2 = reg.merge(merged_names, on=\"student_id\", how=\"left\") \\\n          .merge(courses[[\"course_code\",\"course_title\",\"semester_no\"]].drop_duplicates(),\n                 on=[\"course_code\",\"semester_no\"], how=\"left\")\n\n# Ensure categorical dtypes match what grade model expects; if not, it will still work with heuristics\nCAT = [\"course_type\",\"course_code\"]\nfor c in CAT:\n    if c in reg2.columns:\n        reg2[c] = reg2[c].astype(\"category\")\n\n# AFTER_MID feature list (fallback)\nAFTER_MID = [\n    \"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"CA_qa\",\"Mid_norm\",\n    \"attendance_percentage\",\"lms_logins\",\"forum_posts\",\"chatbot_interactions\",\"on_time_submission_rate\",\n    \"interest_rating\",\"perceived_difficulty\",\"credit_hours\",\"course_type\",\"course_code\"\n]\nFEATURES = [\n    \"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"CA_qa\",\"Mid_norm\",\"Final_norm\",\n    \"attendance_percentage\",\"lms_logins\",\"forum_posts\",\"chatbot_interactions\",\"on_time_submission_rate\",\n    \"interest_rating\",\"perceived_difficulty\",\"credit_hours\",\"prior_sem_avg\",\"course_type\",\"course_code\"\n]\n\nfor _, r in reg2.iterrows():\n    sid = r[\"student_id\"]; cname = r.get(\"name\",\"\")\n    ccode = r[\"course_code\"]; ctitle = r[\"course_title\"]; sem = int(r[\"semester_no\"])\n    attendance = float(r.get(\"attendance_percentage\", 0.0))\n\n    # Build 1-row frames for predictors\n    row_df = pd.DataFrame([r[FEATURES]]).copy()\n    # grade prediction\n    try:\n        pred_final = float(grade_predict(row_df)[0])\n    except Exception:\n        pred_final = float(r.get(\"final_percentage\", 0.0))\n    letter = letter_from_pct(pred_final)\n\n    # risk prediction (encode if we have encoder columns)\n    xr = pd.DataFrame([r[[c for c in AFTER_MID if c in r.index]]]).copy()\n    if risk_encoder_cols is not None:\n        xr_enc = pd.get_dummies(xr, columns=[\"course_type\",\"course_code\"], drop_first=True)\n        # align columns\n        for m in set(risk_encoder_cols) - set(xr_enc.columns):\n            xr_enc[m] = 0\n        xr_enc = xr_enc[risk_encoder_cols]\n    else:\n        # best-effort without encoder cache\n        xr_enc = xr.select_dtypes(include=[np.number]).copy()\n    try:\n        risk_prob = float(risk_predict(xr_enc)[0])\n    except Exception:\n        risk_prob = 0.0\n\n    # Weak topics (lowest mastery 3) for this course\n    c_topics = topics[topics[\"course_code\"]==ccode][\"topic_name\"].unique().tolist()\n    pm = P_mastery[(P_mastery[\"user_id\"].astype(str)==str(sid)) & (P_mastery[\"skill_name\"].isin(c_topics))].copy()\n    if not pm.empty:\n        weak = pm.sort_values(\"p_mastery\").head(3)\n        weak_topics = \"; \".join([f\"{s} ({p:.2f})\" for s,p in zip(weak[\"skill_name\"], weak[\"p_mastery\"])])\n    else:\n        weak_topics = \"\"\n\n    # Upcoming topics (not seen by this student)\n    seen = (assess[(assess[\"student_id\"]==sid)&(assess[\"course_code\"]==ccode)]\n              .merge(topics, on=[\"course_code\",\"topic_id\"], how=\"left\")[\"topic_name\"]\n              .dropna().unique().tolist())\n    upcoming = [t for t in c_topics if t not in seen][:3]\n    upcoming_topics = \"; \".join(upcoming)\n\n    # Elective recs (top-3)\n    try:\n        recs = recommend_courses(sid, topk=3)\n        elective_recs = \"; \".join([f\"{row.course_code}:{row.course_title}({row.score:.2f})\"\n                                   for _,row in recs.iterrows()])\n    except Exception:\n        elective_recs = \"\"\n\n    export_rows.append({\n        \"run_date\": run_date,\n        \"student_id\": sid,\n        \"name\": cname,\n        \"semester_no\": sem,\n        \"course_code\": ccode,\n        \"course_title\": ctitle,\n        \"attendance_percentage\": attendance,\n        \"pred_final_pct\": round(pred_final,2),\n        \"pred_letter\": letter,\n        \"risk_prob_after_mid\": round(risk_prob,3),\n        \"weak_topics\": weak_topics,\n        \"upcoming_topics\": upcoming_topics,\n        \"elective_recs_top3\": elective_recs\n    })\n\nsnapshot_df = pd.DataFrame(export_rows).sort_values([\"student_id\",\"semester_no\",\"course_code\"])\nout_name = f\"current_state_{run_date}.csv\"\nsnapshot_df.to_csv(out_name, index=False)\nprint(f\"Saved {out_name} with shape {snapshot_df.shape}\")\ndisplay(snapshot_df.head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:15:46.120952Z","iopub.execute_input":"2025-10-07T05:15:46.121296Z","iopub.status.idle":"2025-10-07T05:17:15.794162Z","shell.execute_reply.started":"2025-10-07T05:15:46.121273Z","shell.execute_reply":"2025-10-07T05:17:15.793489Z"}},"outputs":[{"name":"stdout","text":"Saved current_state_2025-10-07.csv with shape (4000, 13)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      run_date student_id         name  semester_no course_code  \\\n0   2025-10-07  CSE240001  Sami Rahman            3    CSE 4301   \n1   2025-10-07  CSE240001  Sami Rahman            3    CSE 4302   \n2   2025-10-07  CSE240001  Sami Rahman            3    CSE 4303   \n3   2025-10-07  CSE240001  Sami Rahman            3    CSE 4304   \n4   2025-10-07  CSE240001  Sami Rahman            3    CSE 4305   \n5   2025-10-07  CSE240001  Sami Rahman            3    CSE 4307   \n6   2025-10-07  CSE240001  Sami Rahman            3    CSE 4308   \n13  2025-10-07  CSE240001  Sami Rahman            3    EEE 4383   \n14  2025-10-07  CSE240001  Sami Rahman            3    EEE 4384   \n18  2025-10-07  CSE240001  Sami Rahman            3   Math 4341   \n7   2025-10-07  CSE240001  Sami Rahman            4    CSE 4402   \n8   2025-10-07  CSE240001  Sami Rahman            4    CSE 4403   \n9   2025-10-07  CSE240001  Sami Rahman            4    CSE 4404   \n10  2025-10-07  CSE240001  Sami Rahman            4    CSE 4405   \n11  2025-10-07  CSE240001  Sami Rahman            4    CSE 4407   \n12  2025-10-07  CSE240001  Sami Rahman            4    CSE 4408   \n15  2025-10-07  CSE240001  Sami Rahman            4    EEE 4483   \n16  2025-10-07  CSE240001  Sami Rahman            4    EEE 4484   \n17  2025-10-07  CSE240001  Sami Rahman            4    Hum 4441   \n19  2025-10-07  CSE240001  Sami Rahman            4   Math 4441   \n\n                                    course_title  attendance_percentage  \\\n0                    Object Oriented Programming               88.26000   \n1                Object Oriented Programming Lab               83.52000   \n2                                Data Structures               88.08000   \n3                            Data Structures Lab               93.50000   \n4         Computer Organization and Architecture               90.90000   \n5                    Database Management Systems               93.03000   \n6                Database Management Systems Lab               92.58000   \n13               Electronic Devices and Circuits               90.61000   \n14           Electronic Devices and Circuits Lab               94.36000   \n18                                Linear Algebra               91.14000   \n7                  Visual Programming Lab (Java)               88.83000   \n8                                     Algorithms               91.68000   \n9                                 Algorithms Lab               75.89000   \n10                   Data and Telecommunications               81.78000   \n11                    System Analysis and Design               94.62000   \n12                System Analysis and Design Lab               79.81000   \n15      Digital Electronics and Pulse Techniques               94.06000   \n16  Digital Electronics and Pulse Techniques Lab               82.82000   \n17                            Engineering Ethics               86.54000   \n19                    Probability and Statistics              100.00000   \n\n    pred_final_pct pred_letter  risk_prob_after_mid  \\\n0         73.81000          A-              0.00200   \n1         77.27000           A              0.00200   \n2         74.64000          A-              0.00200   \n3         84.56000          A+              0.00100   \n4         75.69000           A              0.00100   \n5         71.74000          A-              0.00100   \n6         80.47000          A+              0.00000   \n13        71.04000          A-              0.01600   \n14        74.25000          A-              0.00200   \n18        81.07000          A+              0.00000   \n7         80.90000          A+              0.00100   \n8         73.52000          A-              0.00200   \n9         80.30000          A+              0.00300   \n10        80.84000          A+              0.00100   \n11        82.55000          A+              0.00000   \n12        73.81000          A-              0.00100   \n15        76.66000           A              0.00200   \n16        74.91000          A-              0.00100   \n17        73.05000          A-              0.00300   \n19        79.38000           A              0.00200   \n\n                                          weak_topics  \\\n0   Exceptions (0.80); Templates & STL (0.81); Cla...   \n1   Inheritance/Polymorphism Lab (0.90); File I/O ...   \n2   Arrays & Pointers (0.42); Linked Lists (0.75);...   \n3   Linked List Lab (0.85); Sorting & Hashing Lab ...   \n4   DMA & Buses (0.74); CPU Organization & Registe...   \n5   Relational Model & Algebra (0.55); ER Modeling...   \n6   Transactions Lab (0.77); Indexing & Query Lab ...   \n13  Rectifiers & Regulators (0.78); Operational Am...   \n14  Rectifier/Regulator Labs (0.89); Diode/BJT Lab...   \n18  Eigenvalues & Eigenvectors (0.72); Diagonaliza...   \n7      Java GUI (Swing) (0.50); Event Handling (1.00)   \n8   Dynamic Programming (0.10); Graph Algorithms (...   \n9   Algorithm Implementation Lab (0.94); Complexit...   \n10  Digital/Analog Transmission (0.65); Transmissi...   \n11  Feasibility & Planning (0.86); Testing & Docum...   \n12  Modeling & Documentation Lab (0.97); SAD Case ...   \n15  Flip-Flops & Counters (0.86); Wave Shaping & C...   \n16  Counter/Register Labs (0.92); Logic Family Lab...   \n17  Environmental Ethics (0.75); Moral Reasoning (...   \n19  Discrete RVs & PMFs (0.88); Joint Distribution...   \n\n                                      upcoming_topics elective_recs_top3  \n0              Encapsulation; Inheritance; Namespaces                     \n1                                      OOP Lab Basics                     \n2   Stacks & Queues; Graphs (BFS/DFS); Memory Mana...                     \n3                                     Stack/Queue Lab                     \n4                    ALU & Control Unit; RISC vs CISC                     \n5   Normalization; SQL & Transactions; Indexing & ...                     \n6                                                                         \n13  Frequency Response & Noise; JFET & MOSFET; Pow...                     \n14                                        Op-amp Labs                     \n18  Vector Spaces & Bases; Gram-Schmidt & QR; Dete...                     \n7                             Applets/DB Connectivity                     \n8   Greedy; Backtracking & Branch-Bound; NP-Comple...                     \n9                                                                         \n10  OSI & TCP/IP; Signals & Impairments; Multiplexing                     \n11        Systems Concepts; Logical & Physical Design                     \n12                                                                        \n15  Logic Families; Memory & PLAs; A/D & D/A Conve...                     \n16                              Converter & Wave Labs                     \n17      Risk & Safety; Computer Ethics; Global Issues                     \n19  Probability Basics; Continuous RVs & PDFs; Hyp...                     ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_date</th>\n      <th>student_id</th>\n      <th>name</th>\n      <th>semester_no</th>\n      <th>course_code</th>\n      <th>course_title</th>\n      <th>attendance_percentage</th>\n      <th>pred_final_pct</th>\n      <th>pred_letter</th>\n      <th>risk_prob_after_mid</th>\n      <th>weak_topics</th>\n      <th>upcoming_topics</th>\n      <th>elective_recs_top3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4301</td>\n      <td>Object Oriented Programming</td>\n      <td>88.26000</td>\n      <td>73.81000</td>\n      <td>A-</td>\n      <td>0.00200</td>\n      <td>Exceptions (0.80); Templates &amp; STL (0.81); Cla...</td>\n      <td>Encapsulation; Inheritance; Namespaces</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4302</td>\n      <td>Object Oriented Programming Lab</td>\n      <td>83.52000</td>\n      <td>77.27000</td>\n      <td>A</td>\n      <td>0.00200</td>\n      <td>Inheritance/Polymorphism Lab (0.90); File I/O ...</td>\n      <td>OOP Lab Basics</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4303</td>\n      <td>Data Structures</td>\n      <td>88.08000</td>\n      <td>74.64000</td>\n      <td>A-</td>\n      <td>0.00200</td>\n      <td>Arrays &amp; Pointers (0.42); Linked Lists (0.75);...</td>\n      <td>Stacks &amp; Queues; Graphs (BFS/DFS); Memory Mana...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4304</td>\n      <td>Data Structures Lab</td>\n      <td>93.50000</td>\n      <td>84.56000</td>\n      <td>A+</td>\n      <td>0.00100</td>\n      <td>Linked List Lab (0.85); Sorting &amp; Hashing Lab ...</td>\n      <td>Stack/Queue Lab</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4305</td>\n      <td>Computer Organization and Architecture</td>\n      <td>90.90000</td>\n      <td>75.69000</td>\n      <td>A</td>\n      <td>0.00100</td>\n      <td>DMA &amp; Buses (0.74); CPU Organization &amp; Registe...</td>\n      <td>ALU &amp; Control Unit; RISC vs CISC</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4307</td>\n      <td>Database Management Systems</td>\n      <td>93.03000</td>\n      <td>71.74000</td>\n      <td>A-</td>\n      <td>0.00100</td>\n      <td>Relational Model &amp; Algebra (0.55); ER Modeling...</td>\n      <td>Normalization; SQL &amp; Transactions; Indexing &amp; ...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>CSE 4308</td>\n      <td>Database Management Systems Lab</td>\n      <td>92.58000</td>\n      <td>80.47000</td>\n      <td>A+</td>\n      <td>0.00000</td>\n      <td>Transactions Lab (0.77); Indexing &amp; Query Lab ...</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>EEE 4383</td>\n      <td>Electronic Devices and Circuits</td>\n      <td>90.61000</td>\n      <td>71.04000</td>\n      <td>A-</td>\n      <td>0.01600</td>\n      <td>Rectifiers &amp; Regulators (0.78); Operational Am...</td>\n      <td>Frequency Response &amp; Noise; JFET &amp; MOSFET; Pow...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>EEE 4384</td>\n      <td>Electronic Devices and Circuits Lab</td>\n      <td>94.36000</td>\n      <td>74.25000</td>\n      <td>A-</td>\n      <td>0.00200</td>\n      <td>Rectifier/Regulator Labs (0.89); Diode/BJT Lab...</td>\n      <td>Op-amp Labs</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>3</td>\n      <td>Math 4341</td>\n      <td>Linear Algebra</td>\n      <td>91.14000</td>\n      <td>81.07000</td>\n      <td>A+</td>\n      <td>0.00000</td>\n      <td>Eigenvalues &amp; Eigenvectors (0.72); Diagonaliza...</td>\n      <td>Vector Spaces &amp; Bases; Gram-Schmidt &amp; QR; Dete...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>CSE 4402</td>\n      <td>Visual Programming Lab (Java)</td>\n      <td>88.83000</td>\n      <td>80.90000</td>\n      <td>A+</td>\n      <td>0.00100</td>\n      <td>Java GUI (Swing) (0.50); Event Handling (1.00)</td>\n      <td>Applets/DB Connectivity</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>CSE 4403</td>\n      <td>Algorithms</td>\n      <td>91.68000</td>\n      <td>73.52000</td>\n      <td>A-</td>\n      <td>0.00200</td>\n      <td>Dynamic Programming (0.10); Graph Algorithms (...</td>\n      <td>Greedy; Backtracking &amp; Branch-Bound; NP-Comple...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>CSE 4404</td>\n      <td>Algorithms Lab</td>\n      <td>75.89000</td>\n      <td>80.30000</td>\n      <td>A+</td>\n      <td>0.00300</td>\n      <td>Algorithm Implementation Lab (0.94); Complexit...</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>CSE 4405</td>\n      <td>Data and Telecommunications</td>\n      <td>81.78000</td>\n      <td>80.84000</td>\n      <td>A+</td>\n      <td>0.00100</td>\n      <td>Digital/Analog Transmission (0.65); Transmissi...</td>\n      <td>OSI &amp; TCP/IP; Signals &amp; Impairments; Multiplexing</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>CSE 4407</td>\n      <td>System Analysis and Design</td>\n      <td>94.62000</td>\n      <td>82.55000</td>\n      <td>A+</td>\n      <td>0.00000</td>\n      <td>Feasibility &amp; Planning (0.86); Testing &amp; Docum...</td>\n      <td>Systems Concepts; Logical &amp; Physical Design</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>CSE 4408</td>\n      <td>System Analysis and Design Lab</td>\n      <td>79.81000</td>\n      <td>73.81000</td>\n      <td>A-</td>\n      <td>0.00100</td>\n      <td>Modeling &amp; Documentation Lab (0.97); SAD Case ...</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>EEE 4483</td>\n      <td>Digital Electronics and Pulse Techniques</td>\n      <td>94.06000</td>\n      <td>76.66000</td>\n      <td>A</td>\n      <td>0.00200</td>\n      <td>Flip-Flops &amp; Counters (0.86); Wave Shaping &amp; C...</td>\n      <td>Logic Families; Memory &amp; PLAs; A/D &amp; D/A Conve...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>EEE 4484</td>\n      <td>Digital Electronics and Pulse Techniques Lab</td>\n      <td>82.82000</td>\n      <td>74.91000</td>\n      <td>A-</td>\n      <td>0.00100</td>\n      <td>Counter/Register Labs (0.92); Logic Family Lab...</td>\n      <td>Converter &amp; Wave Labs</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>Hum 4441</td>\n      <td>Engineering Ethics</td>\n      <td>86.54000</td>\n      <td>73.05000</td>\n      <td>A-</td>\n      <td>0.00300</td>\n      <td>Environmental Ethics (0.75); Moral Reasoning (...</td>\n      <td>Risk &amp; Safety; Computer Ethics; Global Issues</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2025-10-07</td>\n      <td>CSE240001</td>\n      <td>Sami Rahman</td>\n      <td>4</td>\n      <td>Math 4441</td>\n      <td>Probability and Statistics</td>\n      <td>100.00000</td>\n      <td>79.38000</td>\n      <td>A</td>\n      <td>0.00200</td>\n      <td>Discrete RVs &amp; PMFs (0.88); Joint Distribution...</td>\n      <td>Probability Basics; Continuous RVs &amp; PDFs; Hyp...</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# ==== Update after event + save fresh snapshot CSV (ALL students/courses) ====\nimport pandas as pd, numpy as np\nfrom datetime import datetime\nfrom pyBKT.models import Model\n\n# ---- Utility: safe letter mapping (if not defined) ----\ntry:\n    letter_from_pct\nexcept NameError:\n    def letter_from_pct(p):\n        return (\"A+\" if p>=80 else \"A\" if p>=75 else \"A-\" if p>=70 else\n                \"B+\" if p>=65 else \"B\" if p>=60 else \"B-\" if p>=55 else\n                \"C+\" if p>=50 else \"C\" if p>=45 else \"D\" if p>=40 else \"F\")\n\n# ---- Recompute BKT mastery (robust) ----\ndef compute_P_mastery(assess_df, topics_df):\n    seq = assess_df.copy()\n    seq[\"correct\"] = (seq[\"raw_score\"]/seq[\"max_score\"] >= 0.6).astype(int)\n    seq = (seq.merge(topics_df, on=[\"course_code\",\"topic_id\"], how=\"left\")\n             .dropna(subset=[\"topic_name\"]))\n    if seq.empty:\n        return pd.DataFrame(columns=[\"user_id\",\"skill_name\",\"p_mastery\"])\n    seq_bkt = (seq.rename(columns={\"student_id\":\"user_id\",\"topic_name\":\"skill_name\"})\n                 [[\"user_id\",\"skill_name\",\"correct\",\"timestamp\"]]\n                 .sort_values([\"user_id\",\"skill_name\",\"timestamp\"]).reset_index(drop=True))\n    seq_bkt[\"user_id\"]    = seq_bkt[\"user_id\"].astype(str)\n    seq_bkt[\"skill_name\"] = seq_bkt[\"skill_name\"].astype(str)\n    seq_bkt[\"seq_idx\"]    = seq_bkt.groupby([\"user_id\",\"skill_name\"]).cumcount()\n    # Fit BKT (you can batch this weekly if desired)\n    bkt = Model(seed=42, num_fits=3)\n    bkt.fit(data=seq_bkt[[\"user_id\",\"skill_name\",\"correct\",\"timestamp\"]])\n    pred = bkt.predict(data=seq_bkt[[\"user_id\",\"skill_name\",\"correct\",\"timestamp\"]]).reset_index(drop=True)\n    # align & extract mastery\n    pred[\"user_id\"]    = seq_bkt[\"user_id\"].values\n    pred[\"skill_name\"] = seq_bkt[\"skill_name\"].values\n    pred[\"seq_idx\"]    = seq_bkt[\"seq_idx\"].values\n    # posterior column variants\n    pcol = None\n    for c in pred.columns:\n        if c.lower().startswith(\"posterior\"):\n            pcol = c; break\n    if pcol is None and \"state_predictions\" in pred.columns:\n        def to_known_prob(v):\n            a = np.array(v, dtype=float).ravel()\n            return float(a[1]) if a.size>=2 else float(a.max()) if a.size else np.nan\n        pred[\"p_mastery\"] = pred[\"state_predictions\"].apply(to_known_prob)\n    elif pcol is not None:\n        pred[\"p_mastery\"] = pred[pcol].astype(float)\n    elif \"correct_predictions\" in pred.columns:\n        pred[\"p_mastery\"] = pred[\"correct_predictions\"].astype(float)  # proxy fallback\n    else:\n        pred[\"p_mastery\"] = np.nan\n    last_idx = pred.groupby([\"user_id\",\"skill_name\"])[\"seq_idx\"].transform(\"max\")\n    latest = pred[pred[\"seq_idx\"]==last_idx].copy()\n    return latest[[\"user_id\",\"skill_name\",\"p_mastery\"]].reset_index(drop=True)\n\n# ---- Build regression table with your policy (10% Att, 15% QA, 25% Mid, 50% Final) ----\ndef build_regression_table(assess_df, engage_df, courses_df, feedback_df):\n    df = assess_df.copy()\n    if \"norm\" not in df.columns:\n        df[\"norm\"] = (df[\"raw_score\"]/df[\"max_score\"]).clip(0,1)\n    piv = df.pivot_table(index=[\"student_id\",\"course_code\",\"semester_no\"],\n                         columns=\"assessment_type\", values=\"norm\", aggfunc=\"mean\").reset_index()\n    for c in [\"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"mid\",\"final\"]:\n        if c not in piv.columns: piv[c] = np.nan\n    piv[\"CA_qa\"]    = piv[[\"quiz1\",\"quiz2\",\"quiz3\",\"assignment\"]].mean(axis=1, skipna=True).fillna(0)\n    piv[\"Mid_norm\"] = piv[\"mid\"].fillna(0)\n    piv[\"Final_norm\"]=piv[\"final\"].fillna(0)\n    # attendance merge\n    att = engage_df.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False)[\"attendance_percentage\"].mean()\n    X = piv.merge(att, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n    X[\"Att\"] = X[\"attendance_percentage\"].fillna(0)/100.0\n    # new-policy final %\n    X[\"final_percentage\"] = (0.10*X[\"Att\"] + 0.15*X[\"CA_qa\"] + 0.25*X[\"Mid_norm\"] + 0.50*X[\"Final_norm\"])*100.0\n    # course meta + feedback\n    X = X.merge(courses_df[[\"course_code\",\"semester_no\",\"course_type\",\"credit_hours\"]].drop_duplicates(),\n                on=[\"course_code\",\"semester_no\"], how=\"left\")\n    fb = feedback_df.groupby([\"student_id\",\"course_code\",\"semester_no\"], as_index=False).agg({\n        \"interest_rating\":\"mean\",\"perceived_difficulty\":\"mean\"\n    })\n    X = X.merge(fb, on=[\"student_id\",\"course_code\",\"semester_no\"], how=\"left\")\n    # prior semester average\n    prior = (X[[\"student_id\",\"semester_no\",\"final_percentage\"]]\n             .rename(columns={\"semester_no\":\"prior_sem\",\"final_percentage\":\"prior_pct\"}))\n    prior_gpa = (prior.groupby([\"student_id\",\"prior_sem\"], as_index=False)[\"prior_pct\"]\n                      .mean().rename(columns={\"prior_sem\":\"semester_no\",\"prior_pct\":\"prior_sem_avg\"}))\n    X = X.merge(prior_gpa, on=[\"student_id\",\"semester_no\"], how=\"left\").fillna(0)\n    return X\n\n# ---- Risk prediction helper (uses trained risk_model if available; else heuristic) ----\ndef predict_risk_rows(rows_df, encoder_cols=None):\n    AFTER_MID = [\n        \"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"CA_qa\",\"Mid_norm\",\n        \"attendance_percentage\",\"lms_logins\",\"forum_posts\",\"chatbot_interactions\",\"on_time_submission_rate\",\n        \"interest_rating\",\"perceived_difficulty\",\"credit_hours\",\"course_type\",\"course_code\"\n    ]\n    xr = rows_df[[c for c in AFTER_MID if c in rows_df.columns]].copy()\n    try:\n        # use trained logistic if present\n        xr_enc = pd.get_dummies(xr, columns=[\"course_type\",\"course_code\"], drop_first=True)\n        if encoder_cols is not None:\n            for m in set(encoder_cols) - set(xr_enc.columns):\n                xr_enc[m] = 0\n            xr_enc = xr_enc[encoder_cols]\n        return risk_model.predict_proba(xr_enc)[:,1]\n    except Exception:\n        # heuristic fallback (attendance+CA+Mid blended)\n        approx = (0.10*xr.get(\"attendance_percentage\",0)/100.0 +\n                  0.15*xr.get(\"CA_qa\",0) + 0.25*xr.get(\"Mid_norm\",0))*100.0\n        return (approx < 60).astype(float).values\n\n# ---- Grade prediction helper (uses grade_model if available; else formula) ----\ndef predict_grade_rows(rows_df):\n    FEATURES = [\n        \"quiz1\",\"quiz2\",\"quiz3\",\"assignment\",\"CA_qa\",\"Mid_norm\",\"Final_norm\",\n        \"attendance_percentage\",\"lms_logins\",\"forum_posts\",\"chatbot_interactions\",\"on_time_submission_rate\",\n        \"interest_rating\",\"perceived_difficulty\",\"credit_hours\",\"prior_sem_avg\",\"course_type\",\"course_code\"\n    ]\n    X = rows_df[[c for c in FEATURES if c in rows_df.columns]].copy()\n    try:\n        return grade_model.predict(X)\n    except Exception:\n        return (0.10*rows_df[\"attendance_percentage\"].fillna(0)/100.0 +\n                0.15*rows_df[\"CA_qa\"].fillna(0) +\n                0.25*rows_df[\"Mid_norm\"].fillna(0) +\n                0.50*rows_df[\"Final_norm\"].fillna(0)) * 100.0\n\n# ---- Export snapshot for ALL students/courses ----\ndef save_snapshot_csv(reg_df, P_mastery_df, filename=None):\n    run_date = datetime.utcnow().date().isoformat()\n    out_name = filename or f\"current_state_{run_date}.csv\"\n    names = students[[\"student_id\",\"name\"]]\n    merged = (reg_df.merge(names, on=\"student_id\", how=\"left\")\n                    .merge(courses[[\"course_code\",\"course_title\",\"semester_no\"]].drop_duplicates(),\n                           on=[\"course_code\",\"semester_no\"], how=\"left\"))\n    # predictions\n    merged[\"pred_final_pct\"] = predict_grade_rows(merged).astype(float)\n    merged[\"pred_letter\"]    = merged[\"pred_final_pct\"].apply(letter_from_pct)\n    # risk (align to training encoder if available)\n    try:\n        risk_encoder_cols = list(Xr_enc.columns)  # from earlier training cell\n    except Exception:\n        risk_encoder_cols = None\n    merged[\"risk_prob_after_mid\"] = predict_risk_rows(merged, encoder_cols=risk_encoder_cols)\n    # weak topics (join mastery)\n    def weak_topics_for(row):\n        c_topics = topics[topics[\"course_code\"]==row[\"course_code\"]][\"topic_name\"].unique().tolist()\n        pm = P_mastery_df[(P_mastery_df[\"user_id\"].astype(str)==str(row[\"student_id\"])) &\n                          (P_mastery_df[\"skill_name\"].isin(c_topics))]\n        if pm.empty: return \"\"\n        w = pm.sort_values(\"p_mastery\").head(3)\n        return \"; \".join([f\"{s}({p:.2f})\" for s,p in zip(w[\"skill_name\"], w[\"p_mastery\"])])\n    merged[\"weak_topics\"] = merged.apply(weak_topics_for, axis=1)\n    # upcoming topics (not yet attempted)\n    def upcoming_for(row):\n        seen = (assess[(assess[\"student_id\"]==row[\"student_id\"]) & (assess[\"course_code\"]==row[\"course_code\"])]\n                .merge(topics, on=[\"course_code\",\"topic_id\"], how=\"left\")[\"topic_name\"].dropna().unique().tolist())\n        all_t = topics[topics[\"course_code\"]==row[\"course_code\"]][\"topic_name\"].tolist()\n        up = [t for t in all_t if t not in seen][:3]\n        return \"; \".join(up)\n    merged[\"upcoming_topics\"] = merged.apply(upcoming_for, axis=1)\n    # select/export\n    out = merged[[\n        \"student_id\",\"name\",\"semester_no\",\"course_code\",\"course_title\",\n        \"attendance_percentage\",\"pred_final_pct\",\"pred_letter\",\n        \"risk_prob_after_mid\",\"weak_topics\",\"upcoming_topics\"\n    ]].copy()\n    out.insert(0, \"run_date\", run_date)\n    out.to_csv(out_name, index=False)\n    print(f\"Saved {out_name} with shape {out.shape}\")\n    return out_name, out\n\n# ---- Main: call this when a NEW EVENT arrives ----\ndef on_new_event_and_snapshot(new_row: dict, refit_bkt: bool = True):\n    \"\"\"\n    new_row: {\n      'student_id', 'course_code', 'semester_no', 'assessment_type',\n      'topic_id', 'raw_score', 'max_score', 'timestamp'\n    }\n    \"\"\"\n    global assess, reg, P_mastery\n    # 1) append event\n    assess = pd.concat([assess, pd.DataFrame([new_row])], ignore_index=True)\n    if \"norm\" not in assess.columns:\n        assess[\"norm\"] = (assess[\"raw_score\"]/assess[\"max_score\"]).clip(0,1)\n    else:\n        assess[\"norm\"] = (assess[\"raw_score\"]/assess[\"max_score\"]).clip(0,1)\n\n    # 2) recompute mastery (full refit or skip for speed)\n    if refit_bkt:\n        try:\n            P_mastery = compute_P_mastery(assess, topics)\n        except Exception as e:\n            print(\"BKT refit failed, falling back to correctness-rate proxy:\", e)\n            tmp = (assess.merge(topics, on=[\"course_code\",\"topic_id\"], how=\"left\")\n                        .dropna(subset=[\"topic_name\"]))\n            tmp[\"correct\"] = (tmp[\"raw_score\"]/tmp[\"max_score\"] >= 0.6).astype(int)\n            P_mastery = (tmp.groupby([\"student_id\",\"topic_name\"], as_index=False)[\"correct\"]\n                           .mean().rename(columns={\"student_id\":\"user_id\",\"topic_name\":\"skill_name\",\"correct\":\"p_mastery\"}))\n\n    # 3) rebuild regression table (features + new-policy label)\n    reg = build_regression_table(assess, engage, courses, feedback)\n\n    # 4) save a fresh snapshot CSV for all students/courses\n    return save_snapshot_csv(reg, P_mastery)\n\n# ------------------ Example usage ------------------\n# demo_new = {\n#     \"student_id\": students[\"student_id\"].iloc[0],\n#     \"course_code\": courses[\"course_code\"].iloc[0],\n#     \"semester_no\": int(courses[\"semester_no\"].iloc[0]),\n#     \"assessment_type\": \"quiz2\",\n#     \"topic_id\": topics[topics[\"course_code\"]==courses[\"course_code\"].iloc[0]][\"topic_id\"].iloc[0],\n#     \"raw_score\": 8, \"max_score\": 10, \"timestamp\": pd.Timestamp.utcnow()\n# }\n# fname, snapshot = on_new_event_and_snapshot(demo_new, refit_bkt=True)\n# snapshot.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T05:17:43.713241Z","iopub.execute_input":"2025-10-07T05:17:43.713521Z","iopub.status.idle":"2025-10-07T05:17:43.741749Z","shell.execute_reply.started":"2025-10-07T05:17:43.713500Z","shell.execute_reply":"2025-10-07T05:17:43.741091Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}